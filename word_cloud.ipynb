{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get the data and clean it up a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti = pd.read_csv('data/train.csv')\n",
    "\n",
    "# List of mbti types \n",
    "type_labels = ['ISTJ', 'ISFJ', 'INFJ', 'INTJ', \n",
    "               'ISTP', 'ISFP', 'INFP', 'INTP', \n",
    "               'ESTP', 'ESFP', 'ENFP', 'ENTP', \n",
    "               'ESTJ', 'ESFJ', 'ENFJ', 'ENTJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mbti = []\n",
    "for i, row in mbti.iterrows():\n",
    "    for post in row['posts'].split('|||'):\n",
    "        all_mbti.append([row['type'], post])\n",
    "all_mbti = pd.DataFrame(all_mbti, columns=['type', 'post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316548, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many rows do we have now?\n",
    "all_mbti.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r'url-web'\n",
    "all_mbti['post'] = all_mbti['post'].replace(to_replace = pattern_url, value = subs_url, regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we make everything lower case to remove some noise from capitalisation\n",
    "all_mbti['post'] = all_mbti['post'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "# these are the chars that count as punctuation. Let's remove the punctuation\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(post):\n",
    "    return ''.join([l for l in post if l not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mbti['post'] = all_mbti['post'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Tokenising](http://www.nltk.org/howto/tokenize.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer divides text into a sequence of tokens, which roughly correspond to \"words\". (see the [Stanford Tokeniser](https://nlp.stanford.edu/software/tokenizer.shtml))  We will use tokenisers to clean up the data, making it ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/wahe3bru/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the TreeBankWordTokenizer since it is MUCH quicker than the word_tokenise function\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "all_mbti['tokens'] = all_mbti['post'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Lemmatization](https://pythonprogramming.net/lemmatizing-nltk-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/wahe3bru/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbti_lemma(words, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize all words in dataframe\n",
    "all_mbti['lemma'] = all_mbti['tokens'].apply(mbti_lemma, args=(lemmatizer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(all_mbti.iloc[268702]['tokens']):    \n",
    "    print ('{:20s} --> {:10s}'.format(t, all_mbti.iloc[268702]['lemma'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Stop Words](http://johnlaudun.org/20130126-nltk-stopwords/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are words which do not contain important significance to be used in Search Queries. Usually these words are filtered out from search queries because they return a vast amount of unnecessary information.  See this [blog post](http://xpo6.com/list-of-english-stop-words/) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(stopwords.words('english'))[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):    \n",
    "    return [t for t in tokens if t not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's leave the stop words in for now so that we can test the following **Hypothesis**:\n",
    "* Introverts tend to use the word **`I`** more than extroverts\n",
    "* Conversely, Extroverts tend to favour the word **`you`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to run the analysis again without stop words! Be warned, this can take long with the pandas apply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_mbti['stem'] = all_mbti['tokens'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Bag of words](https://www.packtpub.com/mapt/book/application_development/9781849513609/7/ch07lvl1sec73/bag-of-words-feature-extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text feature extraction is the process of transforming what is essentially a list of words into a feature set that is usable by a classifier. The NLTK classifiers expect dict style feature sets, so we must therefore transform our text into a dict. The Bag of Words model is the simplest method; it constructs a word presence feature set from all the words of an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_count(words, word_dict={}):\n",
    "    \"\"\" this function takes in a list of words and returns a dictionary \n",
    "        with each word as a key, and the value represents the number of \n",
    "        times that word appeared\"\"\"\n",
    "    for word in words:\n",
    "        if word in word_dict.keys():\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create a set of dictionaries\n",
    "# one for each of the MBTI types\n",
    "personality = {}\n",
    "for pp in type_labels:\n",
    "    df = all_mbti.groupby('type')\n",
    "    personality[pp] = {}\n",
    "    for row in df.get_group(pp)['tokens']:\n",
    "        personality[pp] = bag_of_words_count(row, personality[pp])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we create a list of all of the unique words...\n",
    "all_words = set()\n",
    "for pp in type_labels:\n",
    "    for word in personality[pp]:\n",
    "        all_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so that we can create a dictionary of bag of words for the whole dataset\n",
    "personality['all'] = {}\n",
    "for pp in type_labels:    \n",
    "    for word in all_words:\n",
    "        if word in personality[pp].keys():\n",
    "            if word in personality['all']:\n",
    "                personality['all'][word] += personality[pp][word]\n",
    "            else:\n",
    "                personality['all'][word] = personality[pp][word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of words that only appear once! Let's remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8206375"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words in total?\n",
    "sum([v for v in personality['all'].values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80839"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words appear only once?\n",
    "len([v for v in personality['all'].values() if v == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4269\n",
      "7581200\n"
     ]
    }
   ],
   "source": [
    "# how many words appear more than 100 times?\n",
    "# how many words of the total does that account for?\n",
    "print (len([v for v in personality['all'].values() if v >= 100]))\n",
    "print (sum([v for v in personality['all'].values() if v >= 100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9238183729113038"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7581200/8206375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using words that appear more than 100 times seems much more useful!  And this accounts for 92% of all the words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_count = 100\n",
    "word_index = [k for k, v in personality['all'].items() if v > max_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create one big data frame with the word counts by personality profile\n",
    "hm = []\n",
    "for p, p_bow in personality.items():\n",
    "    df_bow = pd.DataFrame([(k, v) for k, v in p_bow.items() if k in word_index], columns=['Word', p])\n",
    "    df_bow.set_index('Word', inplace=True)\n",
    "    hm.append(df_bow)\n",
    "\n",
    "# create one big data frame\n",
    "df_bow = pd.concat(hm, axis=1)\n",
    "df_bow.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>INFJ</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>ISTP</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>INFP</th>\n",
       "      <th>INTP</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>8172.0</td>\n",
       "      <td>8044.0</td>\n",
       "      <td>67871</td>\n",
       "      <td>43864</td>\n",
       "      <td>13883.0</td>\n",
       "      <td>11148.0</td>\n",
       "      <td>87712</td>\n",
       "      <td>52115</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>1696.0</td>\n",
       "      <td>31198.0</td>\n",
       "      <td>27403</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>2168.0</td>\n",
       "      <td>8687.0</td>\n",
       "      <td>8881.0</td>\n",
       "      <td>378402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>5143.0</td>\n",
       "      <td>4111.0</td>\n",
       "      <td>39663</td>\n",
       "      <td>30500</td>\n",
       "      <td>8893.0</td>\n",
       "      <td>6131.0</td>\n",
       "      <td>48008</td>\n",
       "      <td>35868</td>\n",
       "      <td>2191.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>16454.0</td>\n",
       "      <td>18994</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>5019.0</td>\n",
       "      <td>6135.0</td>\n",
       "      <td>230247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>5106.0</td>\n",
       "      <td>4607.0</td>\n",
       "      <td>40231</td>\n",
       "      <td>28753</td>\n",
       "      <td>8725.0</td>\n",
       "      <td>6264.0</td>\n",
       "      <td>48996</td>\n",
       "      <td>33005</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>16945.0</td>\n",
       "      <td>17852</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>1223.0</td>\n",
       "      <td>5471.0</td>\n",
       "      <td>5889.0</td>\n",
       "      <td>227371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>4033.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>31932</td>\n",
       "      <td>22780</td>\n",
       "      <td>7124.0</td>\n",
       "      <td>4825.0</td>\n",
       "      <td>40376</td>\n",
       "      <td>26693</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>13847.0</td>\n",
       "      <td>14728</td>\n",
       "      <td>841.0</td>\n",
       "      <td>986.0</td>\n",
       "      <td>3966.0</td>\n",
       "      <td>4748.0</td>\n",
       "      <td>182876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>3827.0</td>\n",
       "      <td>3571.0</td>\n",
       "      <td>31628</td>\n",
       "      <td>21568</td>\n",
       "      <td>6540.0</td>\n",
       "      <td>5153.0</td>\n",
       "      <td>40710</td>\n",
       "      <td>24881</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>15002.0</td>\n",
       "      <td>14236</td>\n",
       "      <td>943.0</td>\n",
       "      <td>988.0</td>\n",
       "      <td>4343.0</td>\n",
       "      <td>4564.0</td>\n",
       "      <td>180693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>2976.0</td>\n",
       "      <td>2475.0</td>\n",
       "      <td>24312</td>\n",
       "      <td>17857</td>\n",
       "      <td>4962.0</td>\n",
       "      <td>3580.0</td>\n",
       "      <td>29576</td>\n",
       "      <td>21372</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>10217.0</td>\n",
       "      <td>11335</td>\n",
       "      <td>650.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>3114.0</td>\n",
       "      <td>3499.0</td>\n",
       "      <td>138561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>2734.0</td>\n",
       "      <td>2186.0</td>\n",
       "      <td>22221</td>\n",
       "      <td>16010</td>\n",
       "      <td>4696.0</td>\n",
       "      <td>3332.0</td>\n",
       "      <td>24971</td>\n",
       "      <td>17197</td>\n",
       "      <td>1396.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>10329.0</td>\n",
       "      <td>10882</td>\n",
       "      <td>653.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>3050.0</td>\n",
       "      <td>3815.0</td>\n",
       "      <td>124762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>2207.0</td>\n",
       "      <td>2033.0</td>\n",
       "      <td>19445</td>\n",
       "      <td>14436</td>\n",
       "      <td>4055.0</td>\n",
       "      <td>2931.0</td>\n",
       "      <td>23445</td>\n",
       "      <td>16396</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>8591.0</td>\n",
       "      <td>8949</td>\n",
       "      <td>521.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>2614.0</td>\n",
       "      <td>2907.0</td>\n",
       "      <td>110758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>2290.0</td>\n",
       "      <td>2046.0</td>\n",
       "      <td>18376</td>\n",
       "      <td>13179</td>\n",
       "      <td>4244.0</td>\n",
       "      <td>2909.0</td>\n",
       "      <td>22537</td>\n",
       "      <td>15708</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>8028.0</td>\n",
       "      <td>8265</td>\n",
       "      <td>469.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>2604.0</td>\n",
       "      <td>104924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>2186.0</td>\n",
       "      <td>1879.0</td>\n",
       "      <td>18237</td>\n",
       "      <td>14293</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>2726.0</td>\n",
       "      <td>21068</td>\n",
       "      <td>15889</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>7769.0</td>\n",
       "      <td>8904</td>\n",
       "      <td>554.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2404.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>104781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ISTJ    ISFJ   INFJ   INTJ     ISTP     ISFP   INFP   INTP    ESTP  \\\n",
       "i     8172.0  8044.0  67871  43864  13883.0  11148.0  87712  52115  3704.0   \n",
       "the   5143.0  4111.0  39663  30500   8893.0   6131.0  48008  35868  2191.0   \n",
       "to    5106.0  4607.0  40231  28753   8725.0   6264.0  48996  33005  2254.0   \n",
       "a     4033.0  3333.0  31932  22780   7124.0   4825.0  40376  26693  1868.0   \n",
       "and   3827.0  3571.0  31628  21568   6540.0   5153.0  40710  24881  1905.0   \n",
       "of    2976.0  2475.0  24312  17857   4962.0   3580.0  29576  21372  1300.0   \n",
       "you   2734.0  2186.0  22221  16010   4696.0   3332.0  24971  17197  1396.0   \n",
       "that  2207.0  2033.0  19445  14436   4055.0   2931.0  23445  16396  1063.0   \n",
       "it    2290.0  2046.0  18376  13179   4244.0   2909.0  22537  15708  1065.0   \n",
       "is    2186.0  1879.0  18237  14293   3704.0   2726.0  21068  15889  1121.0   \n",
       "\n",
       "        ESFP     ENFP   ENTP    ESTJ    ESFJ    ENFJ    ENTJ     all  \n",
       "i     1696.0  31198.0  27403  1856.0  2168.0  8687.0  8881.0  378402  \n",
       "the    937.0  16454.0  18994  1000.0  1200.0  5019.0  6135.0  230247  \n",
       "to     972.0  16945.0  17852  1078.0  1223.0  5471.0  5889.0  227371  \n",
       "a      796.0  13847.0  14728   841.0   986.0  3966.0  4748.0  182876  \n",
       "and    834.0  15002.0  14236   943.0   988.0  4343.0  4564.0  180693  \n",
       "of     557.0  10217.0  11335   650.0   779.0  3114.0  3499.0  138561  \n",
       "you    651.0  10329.0  10882   653.0   639.0  3050.0  3815.0  124762  \n",
       "that   550.0   8591.0   8949   521.0   615.0  2614.0  2907.0  110758  \n",
       "it     434.0   8028.0   8265   469.0   490.0  2280.0  2604.0  104924  \n",
       "is     482.0   7769.0   8904   554.0   565.0  2404.0  3000.0  104781  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the top 10 words that appear most often?\n",
    "df_bow.sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats not very helpful at all, is it! Its very difficult to extract insight from this data.  Lets see if we can use the $chi^2$ test to see whether Introverts favour the word **`I`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_types = [p for p in type_labels if p[0] == 'I']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ISTJ', 'ISFJ', 'INFJ', 'INTJ', 'ISTP', 'ISFP', 'INFP', 'INTP']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow['I'] = df_bow[intro_types].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to percentages\n",
    "for col in ['I', 'all']:\n",
    "    df_bow[col+'_perc'] = df_bow[col] / df_bow[col].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember the chi2 test from the CINDY framework?  This looks at observed versus expected results and lets us know where the greatest differences from expected are.  The bigger the statistic, the greater the difference from expectation.  The formula is \n",
    "\n",
    "$$ùëê‚Ñéùëñ^2 = \\sum{\\frac{(ùëÇùëèùë†ùëíùëüùë£ùëíùëë ‚àíùëíùë•ùëùùëíùëêùë°ùëíùëë)^2}{ùëíùë•ùëùùëíùëêùë°ùëíùëë}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow['chi2'] = np.power((df_bow['I_perc'] - df_bow['all_perc']), 2) / df_bow['all_perc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_perc</th>\n",
       "      <th>all_perc</th>\n",
       "      <th>chi2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>urlweb</th>\n",
       "      <td>0.002971</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infp</th>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infj</th>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infps</th>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infjs</th>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intp</th>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>0.012716</td>\n",
       "      <td>0.012501</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.050350</td>\n",
       "      <td>0.049933</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intps</th>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.013036</td>\n",
       "      <td>0.012836</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          I_perc  all_perc      chi2\n",
       "urlweb  0.002971  0.002756  0.000017\n",
       "infp    0.001318  0.001179  0.000016\n",
       "infj    0.001173  0.001075  0.000009\n",
       "infps   0.000491  0.000435  0.000007\n",
       "infjs   0.000415  0.000366  0.000007\n",
       "intp    0.000988  0.000918  0.000005\n",
       "my      0.012716  0.012501  0.000004\n",
       "i       0.050350  0.049933  0.000003\n",
       "intps   0.000366  0.000332  0.000003\n",
       "in      0.013036  0.012836  0.000003"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow[['I_perc', 'all_perc', 'chi2']][df_bow['I_perc'] > df_bow['all_perc']].sort_values(by='chi2', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there it is! What can we conclude from this:\n",
    "* I is the 8th most introverted word, by expectation\n",
    "* Introverts tend to post more urls than extroverted people too! \n",
    "* The introverted types are more likely to be written by Introverts, maybe because people post about their own types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_perc</th>\n",
       "      <th>all_perc</th>\n",
       "      <th>chi2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>enfp</th>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entp</th>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entps</th>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfps</th>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entj</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfj</th>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estp</th>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entjs</th>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfjs</th>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ne</th>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.016052</td>\n",
       "      <td>0.016463</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7w6</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7w8</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.002517</td>\n",
       "      <td>0.002670</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lol</th>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estps</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>esfj</th>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>0.005860</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>esfp</th>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         I_perc  all_perc      chi2\n",
       "enfp   0.000479  0.000767  0.000108\n",
       "entp   0.000397  0.000666  0.000108\n",
       "entps  0.000119  0.000238  0.000060\n",
       "enfps  0.000137  0.000241  0.000045\n",
       "entj   0.000259  0.000380  0.000038\n",
       "enfj   0.000288  0.000375  0.000020\n",
       "estp   0.000231  0.000304  0.000017\n",
       "entjs  0.000067  0.000111  0.000017\n",
       "d      0.000378  0.000452  0.000012\n",
       "enfjs  0.000076  0.000112  0.000012\n",
       "ne     0.000268  0.000329  0.000011\n",
       "you    0.016052  0.016463  0.000010\n",
       "7w6    0.000020  0.000041  0.000010\n",
       "7w8    0.000012  0.000028  0.000009\n",
       "he     0.002517  0.002670  0.000009\n",
       "lol    0.000775  0.000855  0.000007\n",
       "estps  0.000043  0.000065  0.000007\n",
       "esfj   0.000249  0.000294  0.000007\n",
       "an     0.005860  0.006061  0.000007\n",
       "esfp   0.000231  0.000271  0.000006"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow[['I_perc', 'all_perc', 'chi2']][df_bow['I_perc'] < df_bow['all_perc']].sort_values(by='chi2', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we have done all of that, lets cheat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Praise be to Python...\n",
    "\n",
    "sklearn has a built in text feature extraction module called [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) that will literally do all that work in one line of code!\n",
    "\n",
    "\n",
    "This function converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(all_mbti['post'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the vectorizer (discussion)\n",
    "\n",
    "Thus far, we have been using the default parameters of [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### show default parameters for CountVectorizer\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the vectorizer is worth tuning, just like a model is worth tuning! Here are a few parameters that you might want to tune:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **stop_words:** string {'english'}, list, or None (default)\n",
    "    - If 'english', a built-in stop word list for English is used.\n",
    "    - If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "    - If None, no stop words will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove English stop words\n",
    "vect = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ngram_range:** tuple (min_n, max_n), default=(1, 1)\n",
    "    - The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
    "    - All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **max_df:** float in range [0.0, 1.0] or int, default=1.0\n",
    "    - When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "    - If float, the parameter represents a proportion of documents.\n",
    "    - If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore terms that appear in more than 50% of the documents\n",
    "vect = CountVectorizer(max_df=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "    - When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. (This value is also called \"cut-off\" in the literature.)\n",
    "    - If float, the parameter represents a proportion of documents.\n",
    "    - If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep terms that appear in at least 2 documents\n",
    "vect = CountVectorizer(min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guidelines for tuning CountVectorizer:**\n",
    "\n",
    "- Use your knowledge of the **problem** and the **text**, and your understanding of the **tuning parameters**, to help you decide what parameters to tune and how to tune them.\n",
    "- **Experiment**, and let the data tell you the best approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "betterVect = CountVectorizer(stop_words='english', \n",
    "                             min_df=2, \n",
    "                             max_df=0.5, \n",
    "                             ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betterVect.fit(all_mbti['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50426"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(betterVect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ngrams](http://www.nltk.org/api/nltk.html?highlight=n%20grams#nltk.util.ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While individual words do carry meaning, it is often the case that combinations of words change meanings of sentences entirely.  For example, what difference does removing the `not` from this sentence make?\n",
    "\n",
    "Natural Language processing is **not** easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngrams are a method to extract combinations of words into features for model buildiing.  The `n` in ngrams specifies the number of tokens to include.  For example, a 2-gram returns all the consecutive pairs of words in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_grams(words, min_n=1, max_n=4):\n",
    "    s = []\n",
    "    for n in range(min_n, max_n):\n",
    "        for ngram in ngrams(words, n):\n",
    "            s.append(' '.join(str(i) for i in ngram))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'two', 'three', 'four', 'one two', 'two three', 'three four', 'one two three', 'two three four']\n"
     ]
    }
   ],
   "source": [
    "print (word_grams('one two three four'.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'hate'),\n",
       " ('hate', 'april'),\n",
       " ('april', 'fools'),\n",
       " ('fools', 'day'),\n",
       " ('day', 'angry'),\n",
       " ('angry', 'theres'),\n",
       " ('theres', 'a'),\n",
       " ('a', 'site'),\n",
       " ('site', 'im'),\n",
       " ('im', 'regularly'),\n",
       " ('regularly', 'on'),\n",
       " ('on', 'and'),\n",
       " ('and', 'the'),\n",
       " ('the', 'admins'),\n",
       " ('admins', 'are'),\n",
       " ('are', 'screwing'),\n",
       " ('screwing', 'everything'),\n",
       " ('everything', 'up'),\n",
       " ('up', 'today'),\n",
       " ('today', 'for'),\n",
       " ('for', 'a'),\n",
       " ('a', 'laugh'),\n",
       " ('laugh', 'but'),\n",
       " ('but', 'i'),\n",
       " ('i', 'dont'),\n",
       " ('dont', 'find'),\n",
       " ('find', 'it'),\n",
       " ('it', 'funny'),\n",
       " ('funny', 'im'),\n",
       " ('im', 'actually'),\n",
       " ('actually', 'quite'),\n",
       " ('quite', 'freaked'),\n",
       " ('freaked', 'out'),\n",
       " ('out', 'about'),\n",
       " ('about', 'it')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in ngrams(all_mbti.iloc[268702]['tokens'], 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'hate', 'april'),\n",
       " ('hate', 'april', 'fools'),\n",
       " ('april', 'fools', 'day'),\n",
       " ('fools', 'day', 'angry'),\n",
       " ('day', 'angry', 'theres'),\n",
       " ('angry', 'theres', 'a'),\n",
       " ('theres', 'a', 'site'),\n",
       " ('a', 'site', 'im'),\n",
       " ('site', 'im', 'regularly'),\n",
       " ('im', 'regularly', 'on'),\n",
       " ('regularly', 'on', 'and'),\n",
       " ('on', 'and', 'the'),\n",
       " ('and', 'the', 'admins'),\n",
       " ('the', 'admins', 'are'),\n",
       " ('admins', 'are', 'screwing'),\n",
       " ('are', 'screwing', 'everything'),\n",
       " ('screwing', 'everything', 'up'),\n",
       " ('everything', 'up', 'today'),\n",
       " ('up', 'today', 'for'),\n",
       " ('today', 'for', 'a'),\n",
       " ('for', 'a', 'laugh'),\n",
       " ('a', 'laugh', 'but'),\n",
       " ('laugh', 'but', 'i'),\n",
       " ('but', 'i', 'dont'),\n",
       " ('i', 'dont', 'find'),\n",
       " ('dont', 'find', 'it'),\n",
       " ('find', 'it', 'funny'),\n",
       " ('it', 'funny', 'im'),\n",
       " ('funny', 'im', 'actually'),\n",
       " ('im', 'actually', 'quite'),\n",
       " ('actually', 'quite', 'freaked'),\n",
       " ('quite', 'freaked', 'out'),\n",
       " ('freaked', 'out', 'about'),\n",
       " ('out', 'about', 'it')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in ngrams(all_mbti.iloc[268702]['tokens'], 3)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
