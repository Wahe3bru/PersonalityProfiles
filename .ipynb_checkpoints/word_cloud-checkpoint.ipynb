{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do machines understand language?\n",
    "\n",
    "[![Natural Language Processing](http://img.youtube.com/vi/yGKTphqxR9Q/0.jpg)](http://www.youtube.com/watch?v=yGKTphqxR9Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial introduces the following key concepts in handling unstructured text data in Python for machine learning.  We will be covering the following concepts:\n",
    "\n",
    "## Individual words analysis\n",
    "* Removing Noise\n",
    "* Tokenising\n",
    "* Stemming \n",
    "* Lemmatisation\n",
    "* Stop Words\n",
    "* n-grams\n",
    "* Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence structure analysis\n",
    "The following concepts are also very useful for text analysis, so please feel free to do more research and see if you can implement these with the MBTI dataset\n",
    "* Part of Speech Tagging\n",
    "* Named Entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, lets get the data and the main library we will be using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [NLTK](http://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK - natural language toolkit - is a leading library for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning (more on this stuff below), wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
    "\n",
    "Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation, NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike. NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.\n",
    "\n",
    "Natural Language Processing with Python provides a practical introduction to programming for language processing. Written by the creators of NLTK, it guides the reader through the fundamentals of writing Python programs, working with corpora, categorizing text, analyzing linguistic structure, and more. The book is being updated for Python 3 and NLTK 3. (The original Python 2 version is still available at http://nltk.org/.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you go through this tutorial, you may get a lookup error! Watch out specifically for the `tokenize` and `stopwords` sections.  Not to worry, this means you will need to download the [corpora](http://www.nltk.org/nltk_data/).  Follow these steps  below to get this! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see this pop up box. Use it to navigate to the correct corpus. \n",
    "\n",
    "NOTE: the box might pop up in the backround, in which case you should use `alt + tab` to switch to the downloader window.\n",
    "![NLTK-downloader](https://github.com/James-Leslie/Jupyter-Notebooks/blob/master/Image-repository/nltk_downloader.png?raw=TRUE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or you can download the corpora directly, eg\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get the data and clean it up a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti = pd.read_csv('data/train.csv')\n",
    "\n",
    "# List of mbti types \n",
    "type_labels = ['ISTJ', 'ISFJ', 'INFJ', 'INTJ', \n",
    "               'ISTP', 'ISFP', 'INFP', 'INTP', \n",
    "               'ESTP', 'ESFP', 'ENFP', 'ENTP', \n",
    "               'ESTJ', 'ESFJ', 'ENFJ', 'ENTJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at how many of the different MBTI types we have data for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEMCAYAAADNtWEcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1103bd518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mbti['type'].value_counts().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have very few samples for the 'ES' types.  Maybe because they are out in the real-world, not sitting behind a computer screen! :)   \n",
    "   \n",
    "Lets increase the size of the dataset by converting each of the 50 posts into the `posts` column into its own row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mbti = []\n",
    "for i, row in mbti.iterrows():\n",
    "    for post in row['posts'].split('|||'):\n",
    "        all_mbti.append([row['type'], post])\n",
    "all_mbti = pd.DataFrame(all_mbti, columns=['type', 'post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316548, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many rows do we have now?\n",
    "all_mbti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I do remember telling people that I'll send them their family members' body parts :tongue:  It's just that I don't think it's scary to joke about that. Either people are too sensitive, or I look...\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mbti.loc[1137]['post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a37bf78c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_mbti['type'].value_counts().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportions of the classes remains the same, but we now have a much larger number of samples for the 'ES' personality types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In text analytics, removing unneccesary information is a key part of getting the data into a usable format.  Some techniques are standard, while your own data will require some creative thinking on your part.\n",
    "\n",
    "For the MBTI data set we will be doing the following steps:\n",
    "* removing the web-urls\n",
    "* making everything lower case\n",
    "* removing punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Regular expressions](https://www.regular-expressions.info/)** can be very useful for extracting information from text.  If you feel brave, go teach yourself all about it... If not, just follow along.  This next step effectively removes all websites and replaces them with the text `'web-url'` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r'url-web'\n",
    "all_mbti['post'] = all_mbti['post'].replace(to_replace = pattern_url, value = subs_url, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'url-web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>url-web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp and intj moments  url-web  sportscenter n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>What has been the most life-changing experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>url-web   url-web  On repeat for most of today.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               post\n",
       "0  INFJ                                           'url-web\n",
       "1  INFJ                                            url-web\n",
       "2  INFJ  enfp and intj moments  url-web  sportscenter n...\n",
       "3  INFJ  What has been the most life-changing experienc...\n",
       "4  INFJ    url-web   url-web  On repeat for most of today."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mbti.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seed of an idea...** There seem to be a lot of YouTube and other links embedded.  Maybe you can think of ways to collect even more information from these links?  How about page titles and names of Youtube videos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we make everything lower case to remove some noise from capitalisation\n",
    "all_mbti['post'] = all_mbti['post'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "# these are the chars that count as punctuation. Let's remove the punctuation\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(post):\n",
    "    return ''.join([l for l in post if l not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mbti['post'] = all_mbti['post'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i hate april fools day angry  theres a site im regularly on and the admins are screwing everything up today for a laugh but i dont find it funny im actually quite freaked out about it '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mbti['post'].iloc[268702]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Tokenising](http://www.nltk.org/howto/tokenize.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer divides text into a sequence of tokens, which roughly correspond to \"words\". (see the [Stanford Tokeniser](https://nlp.stanford.edu/software/tokenizer.shtml))  We will use tokenisers to clean up the data, making it ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/wahe3bru/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'tokenizer',\n",
       " 'divides',\n",
       " 'text',\n",
       " 'into',\n",
       " 'a',\n",
       " 'sequence',\n",
       " 'of',\n",
       " 'tokens',\n",
       " ',',\n",
       " 'which',\n",
       " 'roughly',\n",
       " 'correspond',\n",
       " 'to',\n",
       " '``',\n",
       " 'words',\n",
       " \"''\",\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('A tokenizer divides text into a sequence of tokens, which roughly correspond to \"words\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the TreeBankWordTokenizer since it is MUCH quicker than the word_tokenise function\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "all_mbti['tokens'] = all_mbti['post'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'just',\n",
       " 'cherish',\n",
       " 'the',\n",
       " 'time',\n",
       " 'of',\n",
       " 'solitude',\n",
       " 'bc',\n",
       " 'i',\n",
       " 'revel',\n",
       " 'within',\n",
       " 'my',\n",
       " 'inner',\n",
       " 'world',\n",
       " 'more',\n",
       " 'whereas',\n",
       " 'most',\n",
       " 'other',\n",
       " 'time',\n",
       " 'id',\n",
       " 'be',\n",
       " 'workin',\n",
       " 'just',\n",
       " 'enjoy',\n",
       " 'the',\n",
       " 'me',\n",
       " 'time',\n",
       " 'while',\n",
       " 'you',\n",
       " 'can',\n",
       " 'dont',\n",
       " 'worry',\n",
       " 'people',\n",
       " 'will',\n",
       " 'always',\n",
       " 'be',\n",
       " 'around',\n",
       " 'to']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mbti['tokens'].iloc[19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Stemming](http://www.nltk.org/howto/stem.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of transforming to the root word, that is, it uses an algorithm that removes\n",
    "common word endings from English words, such as “ly,” “es,” “ed,” and “s.” \n",
    "\n",
    "For example, assuming for an analysis you may want to consider “carefully,” “cared,” “cares,” “caringly” as “care” instead of separate words. There are three widely used stemming algorithms, of which we will be using the `SnowballStemmer`:\n",
    "* Porter\n",
    "* Lancaster\n",
    "* Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = 'caring cares cared caringly carefully'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "care\n",
      "care\n",
      "care\n",
      "care\n",
      "care\n"
     ]
    }
   ],
   "source": [
    "# find the stem of each word in words\n",
    "stemmer = SnowballStemmer('english')\n",
    "for word in words.split():\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbti_stemmer(words, stemmer):\n",
    "    return [stemmer.stem(word) for word in words]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem all words in the mbti dataframe\n",
    "all_mbti['stem'] = all_mbti['tokens'].apply(mbti_stemmer, args=(stemmer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i                    --> i         \n",
      "hate                 --> hate      \n",
      "april                --> april     \n",
      "fools                --> fool      \n",
      "day                  --> day       \n",
      "angry                --> angri     \n",
      "theres               --> there     \n",
      "a                    --> a         \n",
      "site                 --> site      \n",
      "im                   --> im        \n",
      "regularly            --> regular   \n",
      "on                   --> on        \n",
      "and                  --> and       \n",
      "the                  --> the       \n",
      "admins               --> admin     \n",
      "are                  --> are       \n",
      "screwing             --> screw     \n",
      "everything           --> everyth   \n",
      "up                   --> up        \n",
      "today                --> today     \n",
      "for                  --> for       \n",
      "a                    --> a         \n",
      "laugh                --> laugh     \n",
      "but                  --> but       \n",
      "i                    --> i         \n",
      "dont                 --> dont      \n",
      "find                 --> find      \n",
      "it                   --> it        \n",
      "funny                --> funni     \n",
      "im                   --> im        \n",
      "actually             --> actual    \n",
      "quite                --> quit      \n",
      "freaked              --> freak     \n",
      "out                  --> out       \n",
      "about                --> about     \n",
      "it                   --> it        \n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(all_mbti.iloc[268702]['tokens']):    \n",
    "    print ('{:20s} --> {:10s}'.format(t, all_mbti.iloc[268702]['stem'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Lemmatization](https://pythonprogramming.net/lemmatizing-nltk-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very similar operation to stemming is called lemmatizing. Lemmatizing is the process of grouping words of similar meaning together. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma.\n",
    "\n",
    "Sometimes you will wind up with a very similar word, but other times you will wind up with a completely different word. Let's see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/wahe3bru/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"ran\",'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbti_lemma(words, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize all words in dataframe\n",
    "all_mbti['lemma'] = all_mbti['tokens'].apply(mbti_lemma, args=(lemmatizer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(all_mbti.iloc[268702]['tokens']):    \n",
    "    print ('{:20s} --> {:10s}'.format(t, all_mbti.iloc[268702]['lemma'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Stop Words](http://johnlaudun.org/20130126-nltk-stopwords/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are words which do not contain important significance to be used in Search Queries. Usually these words are filtered out from search queries because they return a vast amount of unnecessary information.  See this [blog post](http://xpo6.com/list-of-english-stop-words/) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(stopwords.words('english'))[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):    \n",
    "    return [t for t in tokens if t not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's leave the stop words in for now so that we can test the following **Hypothesis**:\n",
    "* Introverts tend to use the word **`I`** more than extroverts\n",
    "* Conversely, Extroverts tend to favour the word **`you`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to run the analysis again without stop words! Be warned, this can take long with the pandas apply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_mbti['stem'] = all_mbti['tokens'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enough with the examples, why are we doing this already?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that pre-processing allows us to finally do some analysis!  lets see what the 20 most common words in the whole text are. (Remember your first coding challenges?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Bag of words](https://www.packtpub.com/mapt/book/application_development/9781849513609/7/ch07lvl1sec73/bag-of-words-feature-extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text feature extraction is the process of transforming what is essentially a list of words into a feature set that is usable by a classifier. The NLTK classifiers expect dict style feature sets, so we must therefore transform our text into a dict. The Bag of Words model is the simplest method; it constructs a word presence feature set from all the words of an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_count(words, word_dict={}):\n",
    "    \"\"\" this function takes in a list of words and returns a dictionary \n",
    "        with each word as a key, and the value represents the number of \n",
    "        times that word appeared\"\"\"\n",
    "    for word in words:\n",
    "        if word in word_dict.keys():\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create a set of dictionaries\n",
    "# one for each of the MBTI types\n",
    "personality = {}\n",
    "for pp in type_labels:\n",
    "    df = all_mbti.groupby('type')\n",
    "    personality[pp] = {}\n",
    "    for row in df.get_group(pp)['tokens']:\n",
    "        personality[pp] = bag_of_words_count(row, personality[pp])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we create a list of all of the unique words...\n",
    "all_words = set()\n",
    "for pp in type_labels:\n",
    "    for word in personality[pp]:\n",
    "        all_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so that we can create a dictionary of bag of words for the whole dataset\n",
    "personality['all'] = {}\n",
    "for pp in type_labels:    \n",
    "    for word in all_words:\n",
    "        if word in personality[pp].keys():\n",
    "            if word in personality['all']:\n",
    "                personality['all'][word] += personality[pp][word]\n",
    "            else:\n",
    "                personality['all'][word] = personality[pp][word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.32661000e+05,   2.30000000e+01,   5.00000000e+00,\n",
       "          2.00000000e+00,   2.00000000e+00,   0.00000000e+00,\n",
       "          2.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00]),\n",
       " array([  1.00000000e+00,   3.78411000e+04,   7.56812000e+04,\n",
       "          1.13521300e+05,   1.51361400e+05,   1.89201500e+05,\n",
       "          2.27041600e+05,   2.64881700e+05,   3.02721800e+05,\n",
       "          3.40561900e+05,   3.78402000e+05]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFJCAYAAACCQLQfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHYVJREFUeJzt3X9M3PXhx/HX9e5AvR9ryTDR6G2ivcTO0BVYk8UDV7MF\nt/ija1rtseCWamcbW4VJB1YpI22txEGWtev8sZolKCDqsvndj7itKkjAam4CEWRLiOlvHW1JvDuF\no9z7+49fLH67sp7X9u7t8/GXHJ/P+Xn52fLkPuvQYYwxAgAAWW/ehb4AAACQHkQdAABLEHUAACxB\n1AEAsARRBwDAEkQdAABLuC70BXxeY2PRtL7fggWXaHz8o7S+ZyawcZeNmyR2ZRMbN0nsynT5+b7/\n+D0+qX+Gy+W80JdwTti4y8ZNEruyiY2bJHZlM6IOAIAliDoAAJYg6gAAWIKoAwBgCaIOAIAliDoA\nAJYg6gAAWIKoAwBgCaIOAIAliDoAAJYg6gAAWIKoAwBgiaz/t7Sl2y0P/OFCX8Kcnq678UJfAgAg\nA/FJHQAASxB1AAAsQdQBALAEUQcAwBJEHQAASxB1AAAsQdQBALAEUQcAwBJEHQAASxB1AAAsQdQB\nALAEUQcAwBJEHQAASxB1AAAsQdQBALAEUQcAwBJEHQAASxB1AAAsQdQBALAEUQcAwBJEHQAASxB1\nAAAsQdQBALDEfxX1gYEBVVZWSpLeffddVVRUqLKyUnfddZeOHTsmSers7NSKFSt0++2369VXX5Uk\nTUxMaOPGjaqoqNDatWt14sQJSVJ/f79WrVql1atXa9euXTN/n127dmnlypVavXq1BgcH0zoUAADb\nueY64KmnntJLL72kiy++WJK0fft21dfX69prr1VHR4eeeuop3X333WptbdWLL76oyclJVVRU6Prr\nr1d7e7uCwaA2btyoP/3pT9q9e7cefvhhNTQ0aOfOnbryyiv14x//WMPDwzLG6M0339Tzzz+vo0eP\nauPGjXrxxRfP+T8AAABsMecn9UAgoJ07d8583dLSomuvvVaSND09rdzcXA0ODmrJkiXKycmRz+dT\nIBDQyMiIIpGISktLJUllZWXq6+tTLBZTIpFQIBCQw+FQKBRSb2+vIpGIQqGQHA6HLr/8ck1PT898\nsgcAAHOb85N6eXm5Dh06NPP1pZdeKkn6xz/+oWeeeUbPPvusXn/9dfl8vpljPB6PYrGYYrHYzOse\nj0fRaFSxWExer3fWsQcPHlRubq7mz58/6/VoNKq8vLwzXt+CBZfI5XL+l3PtkJ/vm/ugNJ6XyWzc\nJLErm9i4SWJXtpoz6qfz5z//Wb/+9a/15JNPKi8vT16vV/F4fOb78XhcPp9v1uvxeFx+v/+0x/r9\nfrnd7tO+x1zGxz9KZUJWGxuLnvU5+fm+lM7LZDZuktiVTWzcJLEr053pB5Oz/tPvf/jDH/TMM8+o\ntbVVV155pSSpsLBQkUhEk5OTikajGh0dVTAYVFFRkbq6uiRJ3d3dKi4ultfrldvt1oEDB2SMUU9P\nj0pKSlRUVKSenh4lk0kdOXJEyWRyzk/pAADgU2f1SX16elrbt2/XZZddpo0bN0qSvvGNb+i+++5T\nZWWlKioqZIxRdXW1cnNzFQ6HVVtbq3A4LLfbrebmZklSY2OjampqND09rVAopMWLF0uSSkpKdMcd\ndyiZTGrLli1pngoAgN0cxhhzoS/i80j3o5Q1j76S1vc7F56uu/Gsz7HlsdOpbNwksSub2LhJYlem\nS+vjdwAAkJmIOgAAliDqAABYgqgDAGAJog4AgCWIOgAAliDqAABYgqgDAGAJog4AgCWIOgAAliDq\nAABYgqgDAGAJog4AgCWIOgAAliDqAABYgqgDAGAJog4AgCWIOgAAliDqAABYgqgDAGAJog4AgCWI\nOgAAliDqAABYgqgDAGAJog4AgCWIOgAAliDqAABYgqgDAGAJog4AgCWIOgAAliDqAABYgqgDAGAJ\nog4AgCWIOgAAliDqAABY4r+K+sDAgCorKyVJ+/fvVzgcVkVFhRoaGpRMJiVJnZ2dWrFihW6//Xa9\n+uqrkqSJiQlt3LhRFRUVWrt2rU6cOCFJ6u/v16pVq7R69Wrt2rVr5u+za9curVy5UqtXr9bg4GBa\nhwIAYLs5o/7UU0/p4Ycf1uTkpCRpx44dqqqqUltbm4wx2rt3r8bGxtTa2qqOjg7t2bNHLS0tSiQS\nam9vVzAYVFtbm5YvX67du3dLkhoaGtTc3Kz29nYNDAxoeHhYQ0NDevPNN/X888+rpaVFjY2N53Y5\nAACWmTPqgUBAO3funPl6aGhIS5culSSVlZWpt7dXg4ODWrJkiXJycuTz+RQIBDQyMqJIJKLS0tKZ\nY/v6+hSLxZRIJBQIBORwOBQKhdTb26tIJKJQKCSHw6HLL79c09PTM5/sAQDA3FxzHVBeXq5Dhw7N\nfG2MkcPhkCR5PB5Fo1HFYjH5fL6ZYzwej2Kx2KzXTz3W6/XOOvbgwYPKzc3V/PnzZ70ejUaVl5d3\nxutbsOASuVzO/3KuHfLzfXMflMbzMpmNmyR2ZRMbN0nsylZzRv2z5s379MN9PB6X3++X1+tVPB6f\n9brP55v1+pmO9fv9crvdp32PuYyPf3S2E7Le2Fj0rM/Jz/eldF4ms3GTxK5sYuMmiV2Z7kw/mJz1\nn35ftGiR9u3bJ0nq7u5WSUmJCgsLFYlENDk5qWg0qtHRUQWDQRUVFamrq2vm2OLiYnm9Xrndbh04\ncEDGGPX09KikpERFRUXq6elRMpnUkSNHlEwm5/yUDgAAPnXWn9Rra2tVX1+vlpYWFRQUqLy8XE6n\nU5WVlaqoqJAxRtXV1crNzVU4HFZtba3C4bDcbream5slSY2NjaqpqdH09LRCoZAWL14sSSopKdEd\nd9yhZDKpLVu2pHcpAACWcxhjzIW+iM8j3Y9S1jz6Slrf71x4uu7Gsz7HlsdOp7Jxk8SubGLjJold\nmS6tj98BAEBmIuoAAFiCqAMAYAmiDgCAJYg6AACWIOoAAFiCqAMAYAmiDgCAJYg6AACWIOoAAFiC\nqAMAYAmiDgCAJYg6AACWIOoAAFiCqAMAYAmiDgCAJYg6AACWIOoAAFiCqAMAYAmiDgCAJYg6AACW\nIOoAAFiCqAMAYAmiDgCAJYg6AACWIOoAAFiCqAMAYAmiDgCAJYg6AACWIOoAAFiCqAMAYAmiDgCA\nJYg6AACWIOoAAFiCqAMAYAlXKidNTU2prq5Ohw8f1rx587R161a5XC7V1dXJ4XBo4cKFamho0Lx5\n89TZ2amOjg65XC6tX79ey5Yt08TEhDZt2qTjx4/L4/GoqalJeXl56u/v1/bt2+V0OhUKhbRhw4Z0\n7wUAwFopfVLv6urSyZMn1dHRoXvvvVe/+MUvtGPHDlVVVamtrU3GGO3du1djY2NqbW1VR0eH9uzZ\no5aWFiUSCbW3tysYDKqtrU3Lly/X7t27JUkNDQ1qbm5We3u7BgYGNDw8nNaxAADYLKWoX3XVVZqe\nnlYymVQsFpPL5dLQ0JCWLl0qSSorK1Nvb68GBwe1ZMkS5eTkyOfzKRAIaGRkRJFIRKWlpTPH9vX1\nKRaLKZFIKBAIyOFwKBQKqbe3N31LAQCwXEqP3y+55BIdPnxY3/3udzU+Pq7HH39cb731lhwOhyTJ\n4/EoGo0qFovJ5/PNnOfxeBSLxWa9fuqxXq931rEHDx6c81oWLLhELpczlRlZKz/fN/dBaTwvk9m4\nSWJXNrFxk8SubJVS1H/7298qFArpgQce0NGjR/XDH/5QU1NTM9+Px+Py+/3yer2Kx+OzXvf5fLNe\nP9Oxfr9/zmsZH/8olQlZbWwsetbn5Of7Ujovk9m4SWJXNrFxk8SuTHemH0xSevzu9/tnPml/6Utf\n0smTJ7Vo0SLt27dPktTd3a2SkhIVFhYqEolocnJS0WhUo6OjCgaDKioqUldX18yxxcXF8nq9crvd\nOnDggIwx6unpUUlJSSqXBwDAF1JKn9R/9KMfafPmzaqoqNDU1JSqq6t13XXXqb6+Xi0tLSooKFB5\nebmcTqcqKytVUVEhY4yqq6uVm5urcDis2tpahcNhud1uNTc3S5IaGxtVU1Oj6elphUIhLV68OK1j\nAQCwmcMYYy70RXwe6X6UsubRV9L6fufC03U3nvU5tjx2OpWNmyR2ZRMbN0nsynRpf/wOAAAyD1EH\nAMASRB0AAEsQdQAALEHUAQCwBFEHAMASRB0AAEsQdQAALEHUAQCwBFEHAMASRB0AAEsQdQAALEHU\nAQCwBFEHAMASRB0AAEsQdQAALEHUAQCwBFEHAMASRB0AAEsQdQAALEHUAQCwBFEHAMASRB0AAEsQ\ndQAALEHUAQCwBFEHAMASRB0AAEsQdQAALEHUAQCwBFEHAMASRB0AAEsQdQAALEHUAQCwBFEHAMAS\nRB0AAEu4Uj3xiSee0CuvvKKpqSmFw2EtXbpUdXV1cjgcWrhwoRoaGjRv3jx1dnaqo6NDLpdL69ev\n17JlyzQxMaFNmzbp+PHj8ng8ampqUl5envr7+7V9+3Y5nU6FQiFt2LAhnVsBALBaSp/U9+3bp7ff\nflvt7e1qbW3V+++/rx07dqiqqkptbW0yxmjv3r0aGxtTa2urOjo6tGfPHrW0tCiRSKi9vV3BYFBt\nbW1avny5du/eLUlqaGhQc3Oz2tvbNTAwoOHh4bSOBQDAZilFvaenR8FgUPfee6/WrVunb33rWxoa\nGtLSpUslSWVlZert7dXg4KCWLFminJwc+Xw+BQIBjYyMKBKJqLS0dObYvr4+xWIxJRIJBQIBORwO\nhUIh9fb2pm8pAACWS+nx+/j4uI4cOaLHH39chw4d0vr162WMkcPhkCR5PB5Fo1HFYjH5fL6Z8zwe\nj2Kx2KzXTz3W6/XOOvbgwYNzXsuCBZfI5XKmMiNr5ef75j4ojedlMhs3SezKJjZuktiVrVKK+vz5\n81VQUKCcnBwVFBQoNzdX77///sz34/G4/H6/vF6v4vH4rNd9Pt+s1890rN/vn/Naxsc/SmVCVhsb\ni571Ofn5vpTOy2Q2bpLYlU1s3CSxK9Od6QeTlB6/FxcX6/XXX5cxRh988IE+/vhjffOb39S+ffsk\nSd3d3SopKVFhYaEikYgmJycVjUY1OjqqYDCooqIidXV1zRxbXFwsr9crt9utAwcOyBijnp4elZSU\npHJ5AAB8IaX0SX3ZsmV66623tHLlShljtGXLFl1xxRWqr69XS0uLCgoKVF5eLqfTqcrKSlVUVMgY\no+rqauXm5iocDqu2tlbhcFhut1vNzc2SpMbGRtXU1Gh6elqhUEiLFy9O61gAAGzmMMaYC30Rn0e6\nH6WsefSVtL7fufB03Y1nfY4tj51OZeMmiV3ZxMZNErsyXdofvwMAgMxD1AEAsARRBwDAEkQdAABL\nEHUAACxB1AEAsARRBwDAEkQdAABLEHUAACxB1AEAsARRBwDAEkQdAABLEHUAACxB1AEAsARRBwDA\nEkQdAABLEHUAACxB1AEAsARRBwDAEkQdAABLEHUAACxB1AEAsARRBwDAEkQdAABLEHUAACxB1AEA\nsARRBwDAEkQdAABLEHUAACxB1AEAsARRBwDAEkQdAABLEHUAACxB1AEAsARRBwDAEp8r6sePH9cN\nN9yg0dFR7d+/X+FwWBUVFWpoaFAymZQkdXZ2asWKFbr99tv16quvSpImJia0ceNGVVRUaO3atTpx\n4oQkqb+/X6tWrdLq1au1a9euzzkNAIAvlpSjPjU1pS1btuiiiy6SJO3YsUNVVVVqa2uTMUZ79+7V\n2NiYWltb1dHRoT179qilpUWJRELt7e0KBoNqa2vT8uXLtXv3bklSQ0ODmpub1d7eroGBAQ0PD6dn\nJQAAXwApR72pqUmrV6/WpZdeKkkaGhrS0qVLJUllZWXq7e3V4OCglixZopycHPl8PgUCAY2MjCgS\niai0tHTm2L6+PsViMSUSCQUCATkcDoVCIfX29qZhIgAAXwyuVE763e9+p7y8PJWWlurJJ5+UJBlj\n5HA4JEkej0fRaFSxWEw+n2/mPI/Ho1gsNuv1U4/1er2zjj148OCc17JgwSVyuZypzMha+fm+uQ9K\n43mZzMZNEruyiY2bJHZlq5Si/uKLL8rhcKivr0/vvvuuamtrZ/53cUmKx+Py+/3yer2Kx+OzXvf5\nfLNeP9Oxfr9/zmsZH/8olQlZbWwsetbn5Of7Ujovk9m4SWJXNrFxk8SuTHemH0xSevz+7LPP6pln\nnlFra6uuvfZaNTU1qaysTPv27ZMkdXd3q6SkRIWFhYpEIpqcnFQ0GtXo6KiCwaCKiorU1dU1c2xx\ncbG8Xq/cbrcOHDggY4x6enpUUlKSyuUBAPCFlNIn9dOpra1VfX29WlpaVFBQoPLycjmdTlVWVqqi\nokLGGFVXVys3N1fhcFi1tbUKh8Nyu91qbm6WJDU2NqqmpkbT09MKhUJavHhxui4PAADrOYwx5kJf\nxOeR7kcpax59Ja3vdy48XXfjWZ9jy2OnU9m4SWJXNrFxk8SuTJf2x+8AACDzEHUAACxB1AEAsARR\nBwDAEkQdAABLEHUAACxB1AEAsARRBwDAEkQdAABLEHUAACxB1AEAsARRBwDAEkQdAABLEHUAACxB\n1AEAsARRBwDAEkQdAABLEHUAACxB1AEAsARRBwDAEkQdAABLEHUAACxB1AEAsARRBwDAEkQdAABL\nEHUAACxB1AEAsARRBwDAEkQdAABLEHUAACxB1AEAsARRBwDAEkQdAABLEHUAACxB1AEAsIQrlZOm\npqa0efNmHT58WIlEQuvXr9c111yjuro6ORwOLVy4UA0NDZo3b546OzvV0dEhl8ul9evXa9myZZqY\nmNCmTZt0/PhxeTweNTU1KS8vT/39/dq+fbucTqdCoZA2bNiQ7r0AAFgrpU/qL730kubPn6+2tjb9\n5je/0datW7Vjxw5VVVWpra1Nxhjt3btXY2Njam1tVUdHh/bs2aOWlhYlEgm1t7crGAyqra1Ny5cv\n1+7duyVJDQ0Nam5uVnt7uwYGBjQ8PJzWsQAA2CylqN900026//77JUnGGDmdTg0NDWnp0qWSpLKy\nMvX29mpwcFBLlixRTk6OfD6fAoGARkZGFIlEVFpaOnNsX1+fYrGYEomEAoGAHA6HQqGQent70zQT\nAAD7pfT43ePxSJJisZjuu+8+VVVVqampSQ6HY+b70WhUsVhMPp9v1nmxWGzW66ce6/V6Zx178ODB\nOa9lwYJL5HI5U5mRtfLzfXMflMbzMpmNmyR2ZRMbN0nsylYpRV2Sjh49qnvvvVcVFRW65ZZb9Nhj\nj818Lx6Py+/3y+v1Kh6Pz3rd5/PNev1Mx/r9/jmvY3z8o1QnZK2xsehZn5Of70vpvExm4yaJXdnE\nxk0SuzLdmX4wSenx+7Fjx7RmzRpt2rRJK1eulCQtWrRI+/btkyR1d3erpKREhYWFikQimpycVDQa\n1ejoqILBoIqKitTV1TVzbHFxsbxer9xutw4cOCBjjHp6elRSUpLK5QEA8IWU0if1xx9/XB9++KF2\n794984fcHnroIW3btk0tLS0qKChQeXm5nE6nKisrVVFRIWOMqqurlZubq3A4rNraWoXDYbndbjU3\nN0uSGhsbVVNTo+npaYVCIS1evDh9SwEAsJzDGGMu9EV8Hul+lLLm0VfS+n7nwtN1N571ObY8djqV\njZskdmUTGzdJ7Mp0aX/8DgAAMg9RBwDAEkQdAABLEHUAACxB1AEAsARRBwDAEkQdAABLEHUAACxB\n1AEAsARRBwDAEkQdAABLEHUAACxB1AEAsARRBwDAEkQdAABLEHUAACxB1AEAsARRBwDAEkQdAABL\nEHUAACxB1AEAsARRBwDAEkQdAABLEHUAACxB1AEAsARRBwDAEkQdAABLEHUAACxB1AEAsARRBwDA\nEkQdAABLEHUAACxB1AEAsARRBwDAEkQdAABLuC70BXxWMpnUz372M/3zn/9UTk6Otm3bpq985SsX\n+rIAAMh4GfdJ/e9//7sSiYSee+45PfDAA3r00Ucv9CUBAJAVMi7qkUhEpaWlkqSvf/3reueddy7w\nFQEAkB0y7vF7LBaT1+ud+drpdOrkyZNyuU5/qfn5vrT+/f+n+ba0vl8mSfc/q0xg4yaJXdnExk0S\nu7JVxn1S93q9isfjM18nk8n/GHQAAPCpjIt6UVGRuru7JUn9/f0KBoMX+IoAAMgODmOMudAXcar/\n+9Pv//rXv2SM0SOPPKKrr776Ql8WAAAZL+OiDgAAUpNxj98BAEBqiDoAAJbgj5V/Ipt+k933v//9\nmf/b3xVXXKF169aprq5ODodDCxcuVENDg+bNm6fOzk51dHTI5XJp/fr1WrZsmSYmJrRp0yYdP35c\nHo9HTU1NysvLU39/v7Zv3y6n06lQKKQNGzactz0DAwP6+c9/rtbWVu3fv/+cbdm1a5dee+01uVwu\nbd68WYWFhedl0/DwsO655x599atflSSFw2F973vfy6pNU1NT2rx5sw4fPqxEIqH169frmmuuyfp7\ndbpdl112Wdbfr+npaT388MN677335HA41NjYqNzc3Ky+X6fbdPLkyay/V2lnYIwx5uWXXza1tbXG\nGGPefvtts27dugt8Rac3MTFhbrvttlmv3XPPPeaNN94wxhhTX19v/vrXv5p///vf5uabbzaTk5Pm\nww8/nPnrp59+2vzyl780xhjzxz/+0WzdutUYY8ytt95q9u/fb5LJpLn77rvN0NDQednz5JNPmptv\nvtmsWrXqnG555513TGVlpUkmk+bw4cNmxYoV521TZ2en2bNnz6xjsm3TCy+8YLZt22aMMWZ8fNzc\ncMMNVtyr0+2y4X797W9/M3V1dcYYY9544w2zbt26rL9fp9tkw71KNx6/fyJbfpPdyMiIPv74Y61Z\ns0Z33nmn+vv7NTQ0pKVLl0qSysrK1Nvbq8HBQS1ZskQ5OTny+XwKBAIaGRmZtbOsrEx9fX2KxWJK\nJBIKBAJyOBwKhULq7e09L3sCgYB27tw58/W52hKJRBQKheRwOHT55ZdrenpaJ06cOC+b3nnnHb32\n2mv6wQ9+oM2bNysWi2Xdpptuukn333+/JMkYI6fTacW9Ot0uG+7Xt7/9bW3dulWSdOTIEfn9/qy/\nX6fbZMO9Sjei/on/9JvsMs1FF12ku+66S3v27FFjY6NqampkjJHD4ZAkeTweRaNRxWIx+Xyf/uYk\nj8ejWCw26/VTjz11+/+9fj6Ul5fP+uVC52rL+dz42U2FhYX66U9/qmeffVZXXnmlfvWrX2XdJo/H\nI6/Xq1gspvvuu09VVVVW3KvT7bLhfkmSy+VSbW2ttm7dqltuucWK+/XZTbbcq3Qi6p/Ilt9kd9VV\nV+nWW2+Vw+HQVVddpfnz5+v48eMz34/H4/L7/f9vTzwel8/nm/X6mY71+/3nb9Qp5s379D+S6dzy\nn97jfPjOd76j6667buavh4eHs3LT0aNHdeedd+q2227TLbfcYs29+uwuW+6XJDU1Nenll19WfX29\nJicn/+tryuRdp24KhULW3Kt0IeqfyJbfZPfCCy/M/JvrPvjgA8ViMV1//fXat2+fJKm7u1slJSUq\nLCxUJBLR5OSkotGoRkdHFQwGVVRUpK6urplji4uL5fV65Xa7deDAARlj1NPTo5KSkguyb9GiRedk\nS1FRkXp6epRMJnXkyBElk0nl5eWdl0133XWXBgcHJUl9fX362te+lnWbjh07pjVr1mjTpk1auXKl\nJDvu1el22XC/fv/73+uJJ56QJF188cVyOBy67rrrsvp+nW7Thg0bsv5epRu/fOYT2fKb7BKJhB58\n8EEdOXJEDodDNTU1WrBggerr6zU1NaWCggJt27ZNTqdTnZ2deu6552SM0T333KPy8nJ9/PHHqq2t\n1djYmNxut5qbm5Wfn6/+/n498sgjmp6eVigUUnV19XnbdOjQIf3kJz9RZ2en3nvvvXO2ZefOneru\n7lYymdSDDz54Tn9wOXXT0NCQtm7dKrfbrS9/+cvaunWrvF5vVm3atm2b/vKXv6igoGDmtYceekjb\ntm3L6nt1ul1VVVV67LHHsvp+ffTRR3rwwQd17NgxnTx5UmvXrtXVV1+d1f/dOt2myy67LOv/u5Vu\nRB0AAEvw+B0AAEsQdQAALEHUAQCwBFEHAMASRB0AAEsQdQAALEHUAQCwBFEHAMAS/wt8qpHXIbOf\ngAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22aa3e43470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets have a look at the distrbution of words\n",
    "plt.hist([v for v in personality['all'].values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of words that only appear once! Let's remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8206375"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words in total?\n",
    "sum([v for v in personality['all'].values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80839"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words appear only once?\n",
    "len([v for v in personality['all'].values() if v == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4269\n",
      "7581200\n"
     ]
    }
   ],
   "source": [
    "# how many words appear more than 100 times?\n",
    "# how many words of the total does that account for?\n",
    "print (len([v for v in personality['all'].values() if v >= 100]))\n",
    "print (sum([v for v in personality['all'].values() if v >= 100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9238183729113038"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7581200/8206375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using words that appear more than 100 times seems much more useful!  And this accounts for 92% of all the words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_count = 100\n",
    "word_index = [k for k, v in personality['all'].items() if v > max_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create one big data frame with the word counts by personality profile\n",
    "hm = []\n",
    "for p, p_bow in personality.items():\n",
    "    df_bow = pd.DataFrame([(k, v) for k, v in p_bow.items() if k in word_index], columns=['Word', p])\n",
    "    df_bow.set_index('Word', inplace=True)\n",
    "    hm.append(df_bow)\n",
    "\n",
    "# create one big data frame\n",
    "df_bow = pd.concat(hm, axis=1)\n",
    "df_bow.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>INFJ</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>ISTP</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>INFP</th>\n",
       "      <th>INTP</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>8172.0</td>\n",
       "      <td>8044.0</td>\n",
       "      <td>67871</td>\n",
       "      <td>43864</td>\n",
       "      <td>13883.0</td>\n",
       "      <td>11148.0</td>\n",
       "      <td>87712</td>\n",
       "      <td>52115</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>1696.0</td>\n",
       "      <td>31198.0</td>\n",
       "      <td>27403</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>2168.0</td>\n",
       "      <td>8687.0</td>\n",
       "      <td>8881.0</td>\n",
       "      <td>378402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>5143.0</td>\n",
       "      <td>4111.0</td>\n",
       "      <td>39663</td>\n",
       "      <td>30500</td>\n",
       "      <td>8893.0</td>\n",
       "      <td>6131.0</td>\n",
       "      <td>48008</td>\n",
       "      <td>35868</td>\n",
       "      <td>2191.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>16454.0</td>\n",
       "      <td>18994</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>5019.0</td>\n",
       "      <td>6135.0</td>\n",
       "      <td>230247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>5106.0</td>\n",
       "      <td>4607.0</td>\n",
       "      <td>40231</td>\n",
       "      <td>28753</td>\n",
       "      <td>8725.0</td>\n",
       "      <td>6264.0</td>\n",
       "      <td>48996</td>\n",
       "      <td>33005</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>16945.0</td>\n",
       "      <td>17852</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>1223.0</td>\n",
       "      <td>5471.0</td>\n",
       "      <td>5889.0</td>\n",
       "      <td>227371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>4033.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>31932</td>\n",
       "      <td>22780</td>\n",
       "      <td>7124.0</td>\n",
       "      <td>4825.0</td>\n",
       "      <td>40376</td>\n",
       "      <td>26693</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>13847.0</td>\n",
       "      <td>14728</td>\n",
       "      <td>841.0</td>\n",
       "      <td>986.0</td>\n",
       "      <td>3966.0</td>\n",
       "      <td>4748.0</td>\n",
       "      <td>182876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>3827.0</td>\n",
       "      <td>3571.0</td>\n",
       "      <td>31628</td>\n",
       "      <td>21568</td>\n",
       "      <td>6540.0</td>\n",
       "      <td>5153.0</td>\n",
       "      <td>40710</td>\n",
       "      <td>24881</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>15002.0</td>\n",
       "      <td>14236</td>\n",
       "      <td>943.0</td>\n",
       "      <td>988.0</td>\n",
       "      <td>4343.0</td>\n",
       "      <td>4564.0</td>\n",
       "      <td>180693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>2976.0</td>\n",
       "      <td>2475.0</td>\n",
       "      <td>24312</td>\n",
       "      <td>17857</td>\n",
       "      <td>4962.0</td>\n",
       "      <td>3580.0</td>\n",
       "      <td>29576</td>\n",
       "      <td>21372</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>10217.0</td>\n",
       "      <td>11335</td>\n",
       "      <td>650.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>3114.0</td>\n",
       "      <td>3499.0</td>\n",
       "      <td>138561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>2734.0</td>\n",
       "      <td>2186.0</td>\n",
       "      <td>22221</td>\n",
       "      <td>16010</td>\n",
       "      <td>4696.0</td>\n",
       "      <td>3332.0</td>\n",
       "      <td>24971</td>\n",
       "      <td>17197</td>\n",
       "      <td>1396.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>10329.0</td>\n",
       "      <td>10882</td>\n",
       "      <td>653.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>3050.0</td>\n",
       "      <td>3815.0</td>\n",
       "      <td>124762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>2207.0</td>\n",
       "      <td>2033.0</td>\n",
       "      <td>19445</td>\n",
       "      <td>14436</td>\n",
       "      <td>4055.0</td>\n",
       "      <td>2931.0</td>\n",
       "      <td>23445</td>\n",
       "      <td>16396</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>8591.0</td>\n",
       "      <td>8949</td>\n",
       "      <td>521.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>2614.0</td>\n",
       "      <td>2907.0</td>\n",
       "      <td>110758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>2290.0</td>\n",
       "      <td>2046.0</td>\n",
       "      <td>18376</td>\n",
       "      <td>13179</td>\n",
       "      <td>4244.0</td>\n",
       "      <td>2909.0</td>\n",
       "      <td>22537</td>\n",
       "      <td>15708</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>8028.0</td>\n",
       "      <td>8265</td>\n",
       "      <td>469.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>2604.0</td>\n",
       "      <td>104924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>2186.0</td>\n",
       "      <td>1879.0</td>\n",
       "      <td>18237</td>\n",
       "      <td>14293</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>2726.0</td>\n",
       "      <td>21068</td>\n",
       "      <td>15889</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>7769.0</td>\n",
       "      <td>8904</td>\n",
       "      <td>554.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2404.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>104781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ISTJ    ISFJ   INFJ   INTJ     ISTP     ISFP   INFP   INTP    ESTP  \\\n",
       "i     8172.0  8044.0  67871  43864  13883.0  11148.0  87712  52115  3704.0   \n",
       "the   5143.0  4111.0  39663  30500   8893.0   6131.0  48008  35868  2191.0   \n",
       "to    5106.0  4607.0  40231  28753   8725.0   6264.0  48996  33005  2254.0   \n",
       "a     4033.0  3333.0  31932  22780   7124.0   4825.0  40376  26693  1868.0   \n",
       "and   3827.0  3571.0  31628  21568   6540.0   5153.0  40710  24881  1905.0   \n",
       "of    2976.0  2475.0  24312  17857   4962.0   3580.0  29576  21372  1300.0   \n",
       "you   2734.0  2186.0  22221  16010   4696.0   3332.0  24971  17197  1396.0   \n",
       "that  2207.0  2033.0  19445  14436   4055.0   2931.0  23445  16396  1063.0   \n",
       "it    2290.0  2046.0  18376  13179   4244.0   2909.0  22537  15708  1065.0   \n",
       "is    2186.0  1879.0  18237  14293   3704.0   2726.0  21068  15889  1121.0   \n",
       "\n",
       "        ESFP     ENFP   ENTP    ESTJ    ESFJ    ENFJ    ENTJ     all  \n",
       "i     1696.0  31198.0  27403  1856.0  2168.0  8687.0  8881.0  378402  \n",
       "the    937.0  16454.0  18994  1000.0  1200.0  5019.0  6135.0  230247  \n",
       "to     972.0  16945.0  17852  1078.0  1223.0  5471.0  5889.0  227371  \n",
       "a      796.0  13847.0  14728   841.0   986.0  3966.0  4748.0  182876  \n",
       "and    834.0  15002.0  14236   943.0   988.0  4343.0  4564.0  180693  \n",
       "of     557.0  10217.0  11335   650.0   779.0  3114.0  3499.0  138561  \n",
       "you    651.0  10329.0  10882   653.0   639.0  3050.0  3815.0  124762  \n",
       "that   550.0   8591.0   8949   521.0   615.0  2614.0  2907.0  110758  \n",
       "it     434.0   8028.0   8265   469.0   490.0  2280.0  2604.0  104924  \n",
       "is     482.0   7769.0   8904   554.0   565.0  2404.0  3000.0  104781  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the top 10 words that appear most often?\n",
    "df_bow.sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats not very helpful at all, is it! Its very difficult to extract insight from this data.  Lets see if we can use the $chi^2$ test to see whether Introverts favour the word **`I`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_types = [p for p in type_labels if p[0] == 'I']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ISTJ', 'ISFJ', 'INFJ', 'INTJ', 'ISTP', 'ISFP', 'INFP', 'INTP']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow['I'] = df_bow[intro_types].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to percentages\n",
    "for col in ['I', 'all']:\n",
    "    df_bow[col+'_perc'] = df_bow[col] / df_bow[col].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember the chi2 test from the CINDY framework?  This looks at observed versus expected results and lets us know where the greatest differences from expected are.  The bigger the statistic, the greater the difference from expectation.  The formula is \n",
    "\n",
    "$$𝑐ℎ𝑖^2 = \\sum{\\frac{(𝑂𝑏𝑠𝑒𝑟𝑣𝑒𝑑 −𝑒𝑥𝑝𝑒𝑐𝑡𝑒𝑑)^2}{𝑒𝑥𝑝𝑒𝑐𝑡𝑒𝑑}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow['chi2'] = np.power((df_bow['I_perc'] - df_bow['all_perc']), 2) / df_bow['all_perc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_perc</th>\n",
       "      <th>all_perc</th>\n",
       "      <th>chi2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>urlweb</th>\n",
       "      <td>0.002971</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infp</th>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infj</th>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infps</th>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infjs</th>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intp</th>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>0.012716</td>\n",
       "      <td>0.012501</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.050350</td>\n",
       "      <td>0.049933</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intps</th>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.013036</td>\n",
       "      <td>0.012836</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          I_perc  all_perc      chi2\n",
       "urlweb  0.002971  0.002756  0.000017\n",
       "infp    0.001318  0.001179  0.000016\n",
       "infj    0.001173  0.001075  0.000009\n",
       "infps   0.000491  0.000435  0.000007\n",
       "infjs   0.000415  0.000366  0.000007\n",
       "intp    0.000988  0.000918  0.000005\n",
       "my      0.012716  0.012501  0.000004\n",
       "i       0.050350  0.049933  0.000003\n",
       "intps   0.000366  0.000332  0.000003\n",
       "in      0.013036  0.012836  0.000003"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow[['I_perc', 'all_perc', 'chi2']][df_bow['I_perc'] > df_bow['all_perc']].sort_values(by='chi2', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there it is! What can we conclude from this:\n",
    "* I is the 8th most introverted word, by expectation\n",
    "* Introverts tend to post more urls than extroverted people too! \n",
    "* The introverted types are more likely to be written by Introverts, maybe because people post about their own types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_perc</th>\n",
       "      <th>all_perc</th>\n",
       "      <th>chi2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>enfp</th>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entp</th>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entps</th>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfps</th>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entj</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfj</th>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estp</th>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entjs</th>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfjs</th>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ne</th>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.016052</td>\n",
       "      <td>0.016463</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7w6</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7w8</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.002517</td>\n",
       "      <td>0.002670</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lol</th>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estps</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>esfj</th>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>0.005860</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>esfp</th>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         I_perc  all_perc      chi2\n",
       "enfp   0.000479  0.000767  0.000108\n",
       "entp   0.000397  0.000666  0.000108\n",
       "entps  0.000119  0.000238  0.000060\n",
       "enfps  0.000137  0.000241  0.000045\n",
       "entj   0.000259  0.000380  0.000038\n",
       "enfj   0.000288  0.000375  0.000020\n",
       "estp   0.000231  0.000304  0.000017\n",
       "entjs  0.000067  0.000111  0.000017\n",
       "d      0.000378  0.000452  0.000012\n",
       "enfjs  0.000076  0.000112  0.000012\n",
       "ne     0.000268  0.000329  0.000011\n",
       "you    0.016052  0.016463  0.000010\n",
       "7w6    0.000020  0.000041  0.000010\n",
       "7w8    0.000012  0.000028  0.000009\n",
       "he     0.002517  0.002670  0.000009\n",
       "lol    0.000775  0.000855  0.000007\n",
       "estps  0.000043  0.000065  0.000007\n",
       "esfj   0.000249  0.000294  0.000007\n",
       "an     0.005860  0.006061  0.000007\n",
       "esfp   0.000231  0.000271  0.000006"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow[['I_perc', 'all_perc', 'chi2']][df_bow['I_perc'] < df_bow['all_perc']].sort_values(by='chi2', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we have done all of that, lets cheat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Praise be to Python...\n",
    "\n",
    "sklearn has a built in text feature extraction module called [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) that will literally do all that work in one line of code!\n",
    "\n",
    "\n",
    "This function converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(all_mbti['post'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the vectorizer (discussion)\n",
    "\n",
    "Thus far, we have been using the default parameters of [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### show default parameters for CountVectorizer\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the vectorizer is worth tuning, just like a model is worth tuning! Here are a few parameters that you might want to tune:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **stop_words:** string {'english'}, list, or None (default)\n",
    "    - If 'english', a built-in stop word list for English is used.\n",
    "    - If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "    - If None, no stop words will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove English stop words\n",
    "vect = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ngram_range:** tuple (min_n, max_n), default=(1, 1)\n",
    "    - The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
    "    - All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **max_df:** float in range [0.0, 1.0] or int, default=1.0\n",
    "    - When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "    - If float, the parameter represents a proportion of documents.\n",
    "    - If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore terms that appear in more than 50% of the documents\n",
    "vect = CountVectorizer(max_df=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "    - When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. (This value is also called \"cut-off\" in the literature.)\n",
    "    - If float, the parameter represents a proportion of documents.\n",
    "    - If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep terms that appear in at least 2 documents\n",
    "vect = CountVectorizer(min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guidelines for tuning CountVectorizer:**\n",
    "\n",
    "- Use your knowledge of the **problem** and the **text**, and your understanding of the **tuning parameters**, to help you decide what parameters to tune and how to tune them.\n",
    "- **Experiment**, and let the data tell you the best approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "betterVect = CountVectorizer(stop_words='english', \n",
    "                             min_df=2, \n",
    "                             max_df=0.5, \n",
    "                             ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betterVect.fit(all_mbti['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50426"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(betterVect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ngrams](http://www.nltk.org/api/nltk.html?highlight=n%20grams#nltk.util.ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While individual words do carry meaning, it is often the case that combinations of words change meanings of sentences entirely.  For example, what difference does removing the `not` from this sentence make?\n",
    "\n",
    "Natural Language processing is **not** easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngrams are a method to extract combinations of words into features for model buildiing.  The `n` in ngrams specifies the number of tokens to include.  For example, a 2-gram returns all the consecutive pairs of words in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_grams(words, min_n=1, max_n=4):\n",
    "    s = []\n",
    "    for n in range(min_n, max_n):\n",
    "        for ngram in ngrams(words, n):\n",
    "            s.append(' '.join(str(i) for i in ngram))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'two', 'three', 'four', 'one two', 'two three', 'three four', 'one two three', 'two three four']\n"
     ]
    }
   ],
   "source": [
    "print (word_grams('one two three four'.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'hate'),\n",
       " ('hate', 'april'),\n",
       " ('april', 'fools'),\n",
       " ('fools', 'day'),\n",
       " ('day', 'angry'),\n",
       " ('angry', 'theres'),\n",
       " ('theres', 'a'),\n",
       " ('a', 'site'),\n",
       " ('site', 'im'),\n",
       " ('im', 'regularly'),\n",
       " ('regularly', 'on'),\n",
       " ('on', 'and'),\n",
       " ('and', 'the'),\n",
       " ('the', 'admins'),\n",
       " ('admins', 'are'),\n",
       " ('are', 'screwing'),\n",
       " ('screwing', 'everything'),\n",
       " ('everything', 'up'),\n",
       " ('up', 'today'),\n",
       " ('today', 'for'),\n",
       " ('for', 'a'),\n",
       " ('a', 'laugh'),\n",
       " ('laugh', 'but'),\n",
       " ('but', 'i'),\n",
       " ('i', 'dont'),\n",
       " ('dont', 'find'),\n",
       " ('find', 'it'),\n",
       " ('it', 'funny'),\n",
       " ('funny', 'im'),\n",
       " ('im', 'actually'),\n",
       " ('actually', 'quite'),\n",
       " ('quite', 'freaked'),\n",
       " ('freaked', 'out'),\n",
       " ('out', 'about'),\n",
       " ('about', 'it')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in ngrams(all_mbti.iloc[268702]['tokens'], 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'hate', 'april'),\n",
       " ('hate', 'april', 'fools'),\n",
       " ('april', 'fools', 'day'),\n",
       " ('fools', 'day', 'angry'),\n",
       " ('day', 'angry', 'theres'),\n",
       " ('angry', 'theres', 'a'),\n",
       " ('theres', 'a', 'site'),\n",
       " ('a', 'site', 'im'),\n",
       " ('site', 'im', 'regularly'),\n",
       " ('im', 'regularly', 'on'),\n",
       " ('regularly', 'on', 'and'),\n",
       " ('on', 'and', 'the'),\n",
       " ('and', 'the', 'admins'),\n",
       " ('the', 'admins', 'are'),\n",
       " ('admins', 'are', 'screwing'),\n",
       " ('are', 'screwing', 'everything'),\n",
       " ('screwing', 'everything', 'up'),\n",
       " ('everything', 'up', 'today'),\n",
       " ('up', 'today', 'for'),\n",
       " ('today', 'for', 'a'),\n",
       " ('for', 'a', 'laugh'),\n",
       " ('a', 'laugh', 'but'),\n",
       " ('laugh', 'but', 'i'),\n",
       " ('but', 'i', 'dont'),\n",
       " ('i', 'dont', 'find'),\n",
       " ('dont', 'find', 'it'),\n",
       " ('find', 'it', 'funny'),\n",
       " ('it', 'funny', 'im'),\n",
       " ('funny', 'im', 'actually'),\n",
       " ('im', 'actually', 'quite'),\n",
       " ('actually', 'quite', 'freaked'),\n",
       " ('quite', 'freaked', 'out'),\n",
       " ('freaked', 'out', 'about'),\n",
       " ('out', 'about', 'it')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in ngrams(all_mbti.iloc[268702]['tokens'], 3)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
